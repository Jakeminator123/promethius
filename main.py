#!/usr/bin/env python3
"""
main.py - Central startpunkt f√∂r poker scraping system

Huvudfunktioner:
- Hanterar initial cleanup och setup
- Startar scraping-loop f√∂r kontinuerlig datainsamling  
- P√• Render: Kombinerar scraping med webserver
- Hanterar graceful shutdown och process-management
"""

from __future__ import annotations
import argparse
import sys
import os
import time
import datetime
import subprocess
import sqlite3
from pathlib import Path
import signal
import threading
from typing import Any

# Import centraliserad path-hantering
sys.path.append(str(Path(__file__).resolve().parent))
from utils.paths import PROJECT_ROOT, POKER_DB, IS_RENDER

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Konfiguration och setup
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

ROOT = PROJECT_ROOT
DB_PATH = POKER_DB.relative_to(ROOT) if not IS_RENDER else POKER_DB

print(f"üè† Projektrot: {ROOT}")
print(f"üíæ Database: {POKER_DB}")

os.chdir(ROOT)
from scrape_hh import scrape  # type: ignore[reportMissingImports]  # noqa: E402

def load_config() -> dict[str, str]:
    """L√§ser konfiguration fr√•n config.txt."""
    kv: dict[str, str] = {}
    with open(ROOT / "config.txt", encoding="utf-8") as fh:
        for line in fh:
            if "=" in line:
                k, v = line.strip().split("=", 1)
                kv[k.strip().upper()] = v.strip()
    return kv

CFG = load_config()
STARTING_DATE = CFG["STARTING_DATE"]

print(f"üåê API: {CFG['BASE_URL']}")
print(f"   Organizer: {CFG['ORGANIZER']}")
print(f"   Event: {CFG['EVENT']}")
print()

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Cleanup och process-hantering
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def kill_old_processes() -> None:
    """F√∂rs√∂ker d√∂da gamla Python-processer (endast f√∂r Render)."""
    if not IS_RENDER:
        return
    
    # psutil inte tillg√§ngligt p√• Render - skulle kunna implementeras senare
    print("‚è≠Ô∏è  Hoppar √∂ver process-cleanup (psutil inte tillg√§ngligt)")

def cleanup_database_locks() -> None:
    """Rensar SQLite WAL/SHM-filer och st√§nger l√•sningar."""
    try:
        from utils.paths import POKER_DB, HEAVY_DB, DB_DIR
        
        print("üîç S√∂ker efter databas-filer...")
        
        # S√∂k efter databas-filer
        search_dirs = [DB_DIR]
        if IS_RENDER:
            search_dirs.append(Path('/var/data'))
        
        all_db_files = []
        patterns = ['poker.db*', 'heavy_analysis.db*']
        
        for search_dir in search_dirs:
            if search_dir.exists():
                for pattern in patterns:
                    all_db_files.extend(search_dir.rglob(pattern))
        
        # L√§gg till k√§nda databas-filer
        known_db_files = [
            POKER_DB, HEAVY_DB,
            POKER_DB.with_suffix('.db-wal'), POKER_DB.with_suffix('.db-shm'),
            HEAVY_DB.with_suffix('.db-wal'), HEAVY_DB.with_suffix('.db-shm'),
        ]
        
        unique_db_files = list(set(known_db_files + all_db_files))
        
        print(f"   üìÅ Hittade {len(unique_db_files)} databas-filer")
        
        # Checkpoint aktiva databaser
        _checkpoint_databases(unique_db_files)
        
        # Radera WAL/SHM-filer
        _remove_lock_files(unique_db_files)
        
    except Exception as e:
        print(f"‚ö†Ô∏è  Fel vid databas-rensning: {e}")

def _checkpoint_databases(db_files: list[Path]) -> None:
    """G√∂r WAL checkpoint p√• alla aktiva databaser."""
    for db_file in db_files:
        if db_file.suffix == '.db' and db_file.exists():
            try:
                conn = sqlite3.connect(str(db_file))
                conn.execute("PRAGMA wal_checkpoint(TRUNCATE)")
                conn.close()
                print(f"   ‚úì Checkpoint: {db_file.name}")
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Checkpoint misslyckades f√∂r {db_file.name}: {e}")

def _remove_lock_files(db_files: list[Path]) -> None:
    """Raderar WAL/SHM l√•sfiler."""
    removed_count = 0
    for db_file in db_files:
        if db_file.name.endswith(('.db-wal', '.db-shm')) and db_file.exists():
            try:
                db_file.unlink()
                print(f"   ‚úì Raderade: {db_file.name}")
                removed_count += 1
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Kunde inte radera {db_file.name}: {e}")
    
    if removed_count > 0:
        print(f"‚úÖ Rensade {removed_count} l√•sfiler")
    else:
        print("‚úÖ Inga l√•sfiler att rensa")

def force_cleanup_on_start() -> None:
    """Tv√•ngsm√§ssig cleanup vid start (endast Render)."""
    if not IS_RENDER:
        return
        
    print("üßπ TV√ÖNGSRENSNING VID START (Render)")
    
    kill_old_processes()
    cleanup_database_locks()
    
    print("‚è±Ô∏è  V√§ntar 5 sekunder...")
    time.sleep(5)
    
    print("‚úÖ Tv√•ngsrensning klar")

def handle_first_deploy() -> bool:
    """Hanterar f√∂rsta deploy p√• Render med total databas-rensning."""
    from utils.paths import POKER_DB, HEAVY_DB, DB_DIR
    
    marker_file = DB_DIR / ".first_deploy_done"
    
    if marker_file.exists():
        print("‚ôªÔ∏è  Kontinuerlig drift - beh√•ller befintlig data")
        try:
            deploy_time = marker_file.read_text().strip()
            print(f"   {deploy_time}")
        except Exception:
            pass
        return True
    
    print("üéâ F√ñRSTA DEPLOYEN - total databas-rensning...")
    
    # Extra s√§kerhet
    kill_old_processes()
    cleanup_database_locks()
    
    # Hitta och radera alla databas-filer
    db_files_deleted = _delete_all_database_files()
    
    print(f"‚úÖ Raderade {db_files_deleted} databas-filer")
    
    # K√∂r full rensning
    if not run_clean_start(skip_on_render=False):
        print("‚ùå KRITISK: F√∂rsta rensning misslyckades")
        return False
    
    # Skapa marker
    marker_file.write_text(f"First deploy completed: {datetime.datetime.now().isoformat()}")
    print("‚úÖ F√∂rsta deployen klar")
    
    return True

def _delete_all_database_files() -> int:
    """Raderar alla databas-filer f√∂r fresh start."""
    from utils.paths import POKER_DB, HEAVY_DB, DB_DIR
    
    search_dirs = [DB_DIR]
    if IS_RENDER:
        search_dirs.append(Path('/var/data'))
    
    all_db_files = []
    patterns = ['poker.db*', 'heavy_analysis.db*']
    
    for search_dir in search_dirs:
        if search_dir.exists():
            for pattern in patterns:
                all_db_files.extend(search_dir.rglob(pattern))
    
    # L√§gg till k√§nda filer
    known_db_files = [
        POKER_DB, HEAVY_DB,
        POKER_DB.with_suffix('.db-wal'), POKER_DB.with_suffix('.db-shm'),
        HEAVY_DB.with_suffix('.db-wal'), HEAVY_DB.with_suffix('.db-shm'),
    ]
    
    unique_db_files = list(set(known_db_files + all_db_files))
    deleted_count = 0
    
    for db_file in unique_db_files:
        if db_file.exists():
            try:
                db_file.unlink()
                print(f"   ‚úì Raderade {db_file.name}")
                deleted_count += 1
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Kunde inte radera {db_file.name}: {e}")
    
    return deleted_count

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Scraping-operationer
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def run_clean_start(skip_on_render: bool = True) -> bool:
    """K√∂r clean_start.py script f√∂r initial rensning."""
    if IS_RENDER and skip_on_render:
        print("üöÄ P√• Render - hoppar √∂ver rensning")
        return True
        
    try:
        print("üßπ K√∂r rensning...")
        result = subprocess.run(
            [sys.executable, "clean_start.py"],
            cwd=ROOT, 
            capture_output=True, 
            text=True, 
            timeout=60
        )

        if result.returncode == 0:
            print("‚úÖ Rensning klar")
            if result.stdout.strip():
                print(f"   {result.stdout.strip()}")
            return True
        else:
            print(f"‚ùå Rensning misslyckades: {result.stderr}")
            return False
            
    except subprocess.TimeoutExpired:
        print("‚è∞ Rensning tog f√∂r l√•ng tid")
        return False
    except Exception as e:
        print(f"‚ùå Fel vid rensning: {e}")
        return False

def run_fetch_process(
    date_str: str, 
    url: str | None, 
    db: str | None,
    skip_scripts: list[str] | None = None, 
    no_scripts: bool = False
) -> None:
    """K√∂r scraping f√∂r ett specifikt datum."""
    try:
        # F√∂rbered arguments f√∂r scrape.py
        argv = ["scrape.py", date_str]
        
        if url:
            argv.extend(["--url", url])
        if db:
            argv.extend(["--db", db])
        if skip_scripts:
            argv.extend(["--skip-scripts"] + skip_scripts)
        if no_scripts:
            argv.append("--no-scripts")

        # K√∂r scrape.main() direkt ist√§llet f√∂r subprocess
        original_argv = sys.argv[:]
        sys.argv = argv
        
        scrape.main()
        
        sys.argv = original_argv

    except KeyboardInterrupt:
        print(f"\n‚èπÔ∏è  Scraping avbrutet f√∂r {date_str}")
        raise
    except Exception as e:
        print(f"‚ùå Fel vid scraping f√∂r {date_str}: {e}")
        raise

def run_single_fetch(
    date_str: str, 
    url: str | None, 
    db: str | None,
    skip_scripts: list[str] | None = None, 
    no_scripts: bool = False
) -> None:
    """K√∂r scraping f√∂r ett enda datum med cleanup f√∂rst."""
    if not run_clean_start():
        print("‚ùå Kan inte forts√§tta utan lyckad rensning")
        return

    run_fetch_process(date_str, url, db, skip_scripts, no_scripts)

def sleep_with_heartbeat(seconds: int, message: str = "V√§ntar...") -> None:
    """Sleep med periodisk loggning f√∂r att h√•lla processen vid liv p√• Render."""
    interval = 30  # Logga var 30:e sekund
    elapsed = 0
    
    while elapsed < seconds:
        remaining = seconds - elapsed
        sleep_time = min(interval, remaining)
        
        if elapsed > 0 and elapsed % 60 == 0:  # Logga varje minut
            print(f"   ‚è±Ô∏è  {message} ({elapsed//60} av {seconds//60} minuter)")
        
        time.sleep(sleep_time)
        elapsed += sleep_time

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Signal handling
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def setup_signal_handlers() -> None:
    """S√§tter upp signal handlers f√∂r graceful shutdown."""
    def signal_handler(signum: int, frame: Any) -> None:
        if IS_RENDER and signum == signal.SIGTERM:
            print("\nüõë SIGTERM p√• Render - graceful shutdown...")
            
            try:
                cleanup_database_locks()
                print("‚úÖ Databas-cleanup klar")
            except Exception:
                pass
            
            print("‚úÖ Graceful shutdown klar")
            sys.exit(0)
        else:
            print(f"\nüõë Signal {signum} - avslutar...")
            sys.exit(0)

    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Huvudloop
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def run_scraping_loop(
    start_date: str, 
    url: str | None, 
    db: str | None,
    sleep_s: int = 300,
    skip_scripts: list[str] | None = None, 
    no_scripts: bool = False,
    no_clean: bool = False, 
    in_thread: bool = False
) -> None:
    """Huvudloop f√∂r kontinuerlig scraping."""
    
    # Initial setup och cleanup
    if not _setup_initial_state(no_clean, in_thread):
        return
    
    # Setup signal handling (endast i main thread)
    if not in_thread:
        setup_signal_handlers()
    else:
        print("üîÑ Scraping-thread: Hoppar √∂ver signal handling")

    # B√∂rja scraping-loop
    day = datetime.date.fromisoformat(start_date)
    batch_size = CFG.get("BATCH_SIZE", "500")
    
    print(f"üîÑ Startar scraping-loop fr√•n {start_date}")
    print(f"   Batch-storlek: {batch_size} h√§nder per dag")

    while True:
        try:
            current_date = day.isoformat()
            
            print(f"üîÑ Scraping f√∂r {current_date}...")
            run_fetch_process(current_date, url, db, skip_scripts, no_scripts)
            print(f"‚úÖ Scraping klar f√∂r {current_date}")

            day += datetime.timedelta(days=1)
            
            # Olika v√§ntetider beroende p√• om vi kommit ikapp dagens datum
            if day == datetime.date.today():
                print("üïë V√§ntar 10 minuter (ikapp dagens datum)...")
                sleep_with_heartbeat(600, "V√§ntar innan n√§sta k√∂rning")
            else:
                print(f"üïë V√§ntar {sleep_s//60} min...")
                next_date = day.isoformat()
                sleep_with_heartbeat(sleep_s, f"V√§ntar innan {next_date}")

        except KeyboardInterrupt:
            print("\n‚èπÔ∏è  Loop avbruten av anv√§ndare")
            break
        except Exception as e:
            print(f"‚ùå Fel i loop f√∂r {day.isoformat()}: {e}")
            print(f"‚è≠Ô∏è  Hoppar √∂ver och forts√§tter...")
            day += datetime.timedelta(days=1)
            time.sleep(5)

def _setup_initial_state(no_clean: bool, in_thread: bool) -> bool:
    """S√§tter upp initial state f√∂r scraping-loop."""
    if IS_RENDER and not no_clean:
        # Render: Hantera f√∂rsta deploy
        if not handle_first_deploy():
            print("‚ùå F√∂rsta deploy misslyckades")
            return False
    elif not no_clean and not run_clean_start():
        # Lokal: Respektera --no-clean flaggan
        print("‚ùå Kan inte forts√§tta utan lyckad rensning")
        return False
    
    return True

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# Render-specifik webserver-hantering
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def start_render_environment(
    start_date: str,
    url: str | None,
    db: str | None, 
    sleep_s: int,
    skip_scripts: list[str],
    no_scripts: bool,
    no_clean: bool
) -> None:
    """Startar scraping + webserver f√∂r Render deployment."""
    
    def run_scraping_background() -> None:
        """K√∂r scraping i bakgrund."""
        print("üîÑ Startar scraping i bakgrund...")
        time.sleep(15)  # L√•t webservern starta f√∂rst
        
        try:
            run_scraping_loop(
                start_date, url, db, sleep_s, skip_scripts, 
                no_scripts, no_clean, in_thread=True
            )
        except Exception as e:
            print(f"‚ùå KRITISKT FEL i scraping-tr√•d: {e}")
            import traceback
            traceback.print_exc()
            print("‚ö†Ô∏è  Scraping-tr√•den dog! Webservern forts√§tter k√∂ra men ingen ny data h√§mtas.")
    
    # Starta scraping i bakgrundsthread
    scraping_thread = threading.Thread(target=run_scraping_background, daemon=True)
    scraping_thread.start()
    print("‚úÖ Scraping startad i bakgrund")
    
    # Skapa databaser om de inte finns
    _ensure_databases_exist()
    
    # Starta webserver som huvudprocess
    _start_webserver()

def _ensure_databases_exist() -> None:
    """Skapar tomma databaser om de inte finns."""
    try:
        from utils.paths import POKER_DB, HEAVY_DB
        
        for db_path in [POKER_DB, HEAVY_DB]:
            if not db_path.exists():
                print(f"üì¶ Skapar tom databas: {db_path.name}")
                db_path.parent.mkdir(parents=True, exist_ok=True)
                conn = sqlite3.connect(str(db_path))
                conn.close()
                
    except Exception as e:
        print(f"‚ö†Ô∏è  Databas-skapande fel: {e}")

def _start_webserver() -> None:
    """Startar webserver som huvudprocess p√• Render."""
    import uvicorn
    
    port = int(os.environ.get("PORT", 8000))
    
    print(f"üåê Webserver som huvudprocess p√• port {port}")
    print(f"üîó URL: https://promethius.onrender.com")
    
    uvicorn.run(
        "app:app",
        host="0.0.0.0",
        port=port,
        reload=False,
        log_level="info",
        access_log=True
    )

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# CLI och main
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

def create_argument_parser() -> argparse.ArgumentParser:
    """Skapar argument parser f√∂r CLI."""
    parser = argparse.ArgumentParser(
        description="Automatisk poker scraping system"
    )
    
    parser.add_argument("--date", help="Startdatum (default fr√•n config.txt)")
    parser.add_argument("--url", help="√ñverskriv BASE_URL")
    parser.add_argument("--db", help="√ñverskriv DB-s√∂kv√§g")
    parser.add_argument("--workers", type=int, default=1,
                       help="Antal worker-processer (default: 1)")
    parser.add_argument("--sleep", type=int, default=300,
                       help="Sovtid mellan k√∂rningar i sekunder (default: 300)")
    parser.add_argument("--skip-scripts", nargs="*", default=[],
                       help="Script att hoppa √∂ver")
    parser.add_argument("--no-scripts", action="store_true",
                       help="Hoppa √∂ver alla processing-scripts")
    parser.add_argument("--no-clean", action="store_true",
                       help="Hoppa √∂ver initial rensning")
    
    return parser

def main() -> None:
    """Huvudfunktion som orchestrerar hela systemet."""
    print("üöÄ Startar automatisk scraping...")
    
    # Tv√•ngsm√§ssig cleanup f√∂rst (Render)
    force_cleanup_on_start()
    
    # Parse arguments
    parser = create_argument_parser()
    args = parser.parse_args()
    
    start_date = args.date or STARTING_DATE
    
    if IS_RENDER:
        # Render: Scraping + webserver
        start_render_environment(
            start_date, args.url, args.db, args.sleep,
            args.skip_scripts, args.no_scripts, args.no_clean
        )
    else:
        # Lokal utveckling: Bara scraping
        run_scraping_loop(
            start_date, args.url, args.db, args.sleep,
            args.skip_scripts, args.no_scripts, args.no_clean,
            in_thread=False
        )

if __name__ == "__main__":
    main()
