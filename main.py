#!/usr/bin/env python3
# main.py ‚Äì central startpunkt som f√∂rst rensar och sedan startar scraping
# P√• Render startar ocks√• webservern f√∂r att ha allt i en robust process

from __future__ import annotations
import argparse
import sys
import os
import time
import datetime
import subprocess
import sqlite3
from pathlib import Path
import signal
import threading
from typing import Any
import psutil  # F√∂r process-hantering
import socket

# Import centraliserad path-hantering
sys.path.append(str(Path(__file__).resolve().parent))
from utils.paths import PROJECT_ROOT, POKER_DB, IS_RENDER

# ‚îÄ‚îÄ 1. Hitta projektroten och f√∂rbered import ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
ROOT = PROJECT_ROOT
DB_PATH = POKER_DB.relative_to(ROOT) if not IS_RENDER else POKER_DB

print(f"üè† Projektrot: {ROOT}")
print(f"üíæ Database: {POKER_DB}")

# ‚îÄ‚îÄ CLEANUP-FUNKTIONER ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def kill_old_processes():
    """D√∂d√§r gamla Python-processer som k√∂r scraping/webserver"""
    if not IS_RENDER:
        return  # Bara p√• Render
        
    try:
        current_pid = os.getpid()
        killed_count = 0
        
        for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
            try:
                # Skippa v√•r egen process
                if proc.info['pid'] == current_pid:
                    continue
                    
                # Leta efter andra Python-processer som k√∂r v√•ra scripts
                if (proc.info['name'] in ['python', 'python3', 'python.exe'] and 
                    proc.info['cmdline'] and 
                    any('main.py' in str(cmd) or 'scrape.py' in str(cmd) or 'app.py' in str(cmd) 
                        for cmd in proc.info['cmdline'])):
                    
                    print(f"üî™ D√∂d√§r gammal process: PID {proc.info['pid']} - {' '.join(proc.info['cmdline'][:3])}")
                    proc.terminate()
                    killed_count += 1
                    
            except (psutil.NoSuchProcess, psutil.AccessDenied):
                continue
                
        if killed_count > 0:
            print(f"‚úÖ D√∂dade {killed_count} gamla processer")
            time.sleep(2)  # V√§nta s√• processer hinner d√∂
        else:
            print("‚úÖ Inga gamla processer att d√∂da")
            
    except Exception as e:
        print(f"‚ö†Ô∏è  Kunde inte d√∂da gamla processer: {e}")

def cleanup_database_locks():
    """Rensar SQLite WAL/SHM-filer och st√§nger l√•sningar - HITTAR ALLA DATABASER"""
    try:
        from utils.paths import POKER_DB, HEAVY_DB, DB_DIR
        
        print("üîç S√∂ker efter ALLA databas-filer rekursivt...")
        
        # Lista √∂ver alla databas-relaterade filer (k√§nda paths)
        known_db_files = [
            POKER_DB,
            HEAVY_DB,
            POKER_DB.with_suffix('.db-wal'),
            POKER_DB.with_suffix('.db-shm'), 
            HEAVY_DB.with_suffix('.db-wal'),
            HEAVY_DB.with_suffix('.db-shm'),
        ]
        
        # S√∂k rekursivt efter ALLA databas-filer i hela data-omr√•det
        search_dirs = [DB_DIR]
        if IS_RENDER:
            search_dirs.append(Path('/var/data'))
        
        all_db_files = []
        for search_dir in search_dirs:
            if search_dir.exists():
                # Hitta alla filer som matchar v√•ra databas-namn
                patterns = ['poker.db*', 'heavy_analysis.db*']
                for pattern in patterns:
                    all_db_files.extend(search_dir.rglob(pattern))
        
        # Kombinera k√§nda + hittade filer
        unique_db_files = list(set(known_db_files + all_db_files))
        
        print(f"   üìÅ Hittade {len(unique_db_files)} databas-relaterade filer")
        for db_file in unique_db_files:
            if db_file.exists():
                print(f"      ‚Ä¢ {db_file}")
        
        # F√∂rs√∂k st√§nga alla SQLite-connections f√∂rst (bara .db-filer)
        for db_file in unique_db_files:
            if db_file.suffix == '.db' and db_file.exists():
                try:
                    # √ñppna kort connection f√∂r att trigga WAL checkpoint
                    conn = sqlite3.connect(str(db_file))
                    conn.execute("PRAGMA wal_checkpoint(TRUNCATE)")
                    conn.close()
                    print(f"   ‚úì Checkpoint: {db_file.name}")
                except Exception as e:
                    print(f"   ‚ö†Ô∏è  Checkpoint misslyckades f√∂r {db_file.name}: {e}")
        
        # Radera WAL/SHM-filer som kan vara l√•sta
        removed_count = 0
        for db_file in unique_db_files:
            if db_file.name.endswith(('.db-wal', '.db-shm')) and db_file.exists():
                try:
                    db_file.unlink()
                    print(f"   ‚úì Raderade l√•st fil: {db_file}")
                    removed_count += 1
                except Exception as e:
                    print(f"   ‚ö†Ô∏è  Kunde inte radera {db_file}: {e}")
        
        if removed_count > 0:
            print(f"‚úÖ Rensade {removed_count} l√•sta databas-filer")
        else:
            print("‚úÖ Inga l√•sta databas-filer att rensa")
            
    except Exception as e:
        print(f"‚ö†Ô∏è  Fel vid databas-rensning: {e}")

def force_cleanup_on_start():
    """Tv√•ngsm√§ssig cleanup vid start - d√∂d√§r allt som kan st√∂ra"""
    if not IS_RENDER:
        return
        
    print("üßπ TV√ÖNGSRENSNING VID START (Render)")
    
    # 1. D√∂da gamla processer f√∂rst
    kill_old_processes()
    
    # 2. Rensa databas-l√•sningar
    cleanup_database_locks()
    
    # 3. Extra v√§ntetid f√∂r att allt ska hinna "s√§tta sig"
    print("‚è±Ô∏è  V√§ntar 5 sekunder s√• allt hinner rensas...")
    time.sleep(5)
    
    print("‚úÖ Tv√•ngsrensning klar - forts√§tter med normal start")

os.chdir(ROOT)
from scrape_hh import scrape  # type: ignore[reportMissingImports]  # noqa: E402

# ‚îÄ‚îÄ 2. Hj√§lpfunktioner ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
def load_config() -> dict[str, str]:
    kv: dict[str, str] = {}
    with open(ROOT / "config.txt", encoding="utf-8") as fh:
        for line in fh:
            if "=" in line:
                k, v = line.strip().split("=", 1)
                kv[k.strip().upper()] = v.strip()
    return kv

CFG = load_config()
STARTING_DATE = CFG["STARTING_DATE"]

print(f"üåê API: {CFG['BASE_URL']}")
print(f"   Organizer: {CFG['ORGANIZER']}")
print(f"   Event: {CFG['EVENT']}")
print()

def run_clean_start(skip_on_render: bool = True) -> bool:
    # P√• Render, skippa rensning om inte explicit beg√§rt
    if IS_RENDER and skip_on_render:
        print("üöÄ P√• Render - hoppar √∂ver rensning (anv√§nd skip_on_render=False f√∂r att tvinga)")
        return True
        
    try:
        print("üßπ K√∂r rensning...")
        result = subprocess.run([sys.executable, "clean_start.py"],
                                cwd=ROOT, capture_output=True, text=True, timeout=60)

        if result.returncode == 0:
            print("‚úÖ Rensning klar")
            if result.stdout.strip():
                print(f"   {result.stdout.strip()}")
            return True
        else:
            print(f"‚ùå Rensning misslyckades: {result.stderr}")
            return False
    except subprocess.TimeoutExpired:
        print("‚è∞ Rensning tog f√∂r l√•ng tid - avbryter")
        return False
    except Exception as e:
        print(f"‚ùå Fel vid rensning: {e}")
        return False

def run_fetch_process(date_str: str, url: str | None, db: str | None,
                      skip_scripts: list[str] | None = None, no_scripts: bool = False) -> None:
    try:
        argv = ["scrape.py", date_str]
        if url:
            argv += ["--url", url]
        if db:
            argv += ["--db", db]
        if skip_scripts:
            argv += ["--skip-scripts"] + skip_scripts
        if no_scripts:
            argv += ["--no-scripts"]

        original_argv = sys.argv[:]
        sys.argv = argv

        scrape.main()

        sys.argv = original_argv

    except KeyboardInterrupt:
        print(f"\n‚èπÔ∏è  Scraping avbrutet f√∂r {date_str}")
        raise
    except Exception as e:
        print(f"‚ùå Fel vid scraping f√∂r {date_str}: {e}")
        raise

def run_single_fetch(date_str: str, url: str | None, db: str | None,
                     skip_scripts: list[str] | None = None, no_scripts: bool = False) -> None:
    if not run_clean_start():
        print("‚ùå Kan inte forts√§tta utan lyckad rensning")
        return

    run_fetch_process(date_str, url, db, skip_scripts, no_scripts)

def run_loop(start_date: str, url: str | None, db: str | None,
             sleep_s: int = 300, max_workers: int = 1,
             skip_scripts: list[str] | None = None, no_scripts: bool = False,
             no_clean: bool = False, in_thread: bool = False) -> None:
    # Smart f√∂rsta-deploy-detektion p√• Render
    if IS_RENDER and not no_clean:
        from utils.paths import POKER_DB, HEAVY_DB, DB_DIR
        
        # Marker-fil f√∂r att veta om f√∂rsta deployen √§r gjord
        marker_file = DB_DIR / ".first_deploy_done"
        
        if not marker_file.exists():
            print("üéâ F√ñRSTA DEPLOYEN - rensar alla databaser f√∂r fresh start...")
            
            # Extra s√§kerhet: D√∂da alla processer och rensa l√•sningar
            kill_old_processes()
            cleanup_database_locks()
            
            # Hitta ALLA databas-filer rekursivt f√∂r total rensning
            print("üîç S√∂ker efter ALLA databas-filer f√∂r total rensning...")
            search_dirs = [DB_DIR]
            if IS_RENDER:
                search_dirs.append(Path('/var/data'))
            
            all_db_files = []
            for search_dir in search_dirs:
                if search_dir.exists():
                    # Hitta alla filer som matchar v√•ra databas-namn
                    patterns = ['poker.db*', 'heavy_analysis.db*']
                    for pattern in patterns:
                        all_db_files.extend(search_dir.rglob(pattern))
            
            # L√§gg till k√§nda filer ocks√•
            known_db_files = [
                POKER_DB,
                HEAVY_DB,
                POKER_DB.with_suffix('.db-wal'),
                POKER_DB.with_suffix('.db-shm'),
                HEAVY_DB.with_suffix('.db-wal'),
                HEAVY_DB.with_suffix('.db-shm'),
            ]
            
            unique_db_files = list(set(known_db_files + all_db_files))
            
            print(f"   üìÅ Hittade {len(unique_db_files)} databas-filer att radera")
            
            deleted_count = 0
            for db_file in unique_db_files:
                if db_file.exists():
                    try:
                        db_file.unlink()
                        print(f"   ‚úì Raderade {db_file}")
                        deleted_count += 1
                    except Exception as e:
                        print(f"   ‚ö†Ô∏è  Kunde inte radera {db_file}: {e}")
            
            print(f"‚úÖ Raderade {deleted_count} databas-filer totalt")
            
            # K√∂r full rensning
            if not run_clean_start(skip_on_render=False):
                print("‚ùå Kritisk: Kan inte starta utan lyckad f√∂rsta rensning")
                sys.exit(1)
            
            # Skapa marker-fil s√• vi vet att f√∂rsta deployen √§r gjord
            marker_file.write_text(f"First deploy completed: {datetime.datetime.now().isoformat()}")
            print("‚úÖ F√∂rsta deployen klar - framtida restarts beh√•ller data")
            
        else:
            print("‚ôªÔ∏è  Inte f√∂rsta deployen - beh√•ller befintlig data (kontinuerlig drift)")
            # L√§s n√§r f√∂rsta deployen gjordes
            try:
                deploy_time = marker_file.read_text().strip()
                print(f"   {deploy_time}")
            except:
                pass
    elif not no_clean and not run_clean_start():
        # Lokal milj√∂ - respektera --no-clean flaggan
        print("‚ùå Kan inte forts√§tta utan lyckad rensning")
        return

    day = datetime.date.fromisoformat(start_date)

    # Signal handling bara i main thread, inte i scraping thread
    if not in_thread:
        def signal_handler(signum: int, frame: Any) -> None:
            if IS_RENDER and signum == signal.SIGTERM:
                # P√• Render: G√∂r ordentlig cleanup innan vi avslutar
                print(f"\nüõë Fick SIGTERM p√• Render - g√∂r ordentlig cleanup...")
                
                try:
                    # St√§ng databas-connections
                    cleanup_database_locks()
                    print("‚úÖ Databas-cleanup klar")
                except:
                    pass
                
                print("‚úÖ Graceful shutdown klar - avslutar")
                sys.exit(0)
            elif signum == signal.SIGTERM:
                print(f"\nüõë Fick SIGTERM - avslutar...")
                sys.exit(0)
            else:
                print(f"\nüõë Fick signal {signum} - st√§nger av gracefully...")
                sys.exit(0)

        signal.signal(signal.SIGINT, signal_handler)
        signal.signal(signal.SIGTERM, signal_handler)
    else:
        print("üîÑ Scraping-thread: Hoppar √∂ver signal handling (bara main thread)")

    batch_size = CFG.get("BATCH_SIZE", "500")
    print(f"üîÑ Startar loop fr√•n {start_date}")
    print(f"   Batch-storlek: {batch_size} h√§nder per dag")

    while True:
        try:
            current_date = day.isoformat()

            # K√∂r scraping synkront f√∂r att undvika extra processfork och s√§nka CPU-toppar
            print(f"üîÑ Startar scraping f√∂r {current_date}...")
            run_fetch_process(current_date, url, db, skip_scripts, no_scripts)
            print(f"‚úÖ Scraping klar f√∂r {current_date}")

            day += datetime.timedelta(days=1)

            if day == datetime.date.today():
                print("üïë V√§ntar 10 minuter innan n√§sta k√∂rning...")
                time.sleep(600)
            else:
                print(f"üïë V√§ntar {sleep_s//60} min innan n√§sta k√∂rning...")
                time.sleep(sleep_s)

        except KeyboardInterrupt:
            print("\n‚èπÔ∏è  Loop avbruten av anv√§ndare")
            break
        except Exception as e:
            print(f"‚ùå Fel i loop: {e}")
            print(f"‚è≠Ô∏è  Hoppar √∂ver {day.isoformat()} och forts√§tter...")
            day += datetime.timedelta(days=1)
            time.sleep(5)

# ‚îÄ‚îÄ 3. CLI-parse ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
ap = argparse.ArgumentParser(
    description="Automatisk poker scraping system"
)
_ = ap.add_argument("--date", help="Startdatum (default = STARTING_DATE fr√•n config.txt)")
_ = ap.add_argument("--url", help="√ñverskriv BASE_URL")
_ = ap.add_argument("--db", help="√ñverskriv DB-s√∂kv√§g")
_ = ap.add_argument("--workers", type=int, default=1,
                    help="Antal worker-processer (default: 1)")
_ = ap.add_argument("--sleep", type=int, default=300,
                    help="Sovtid i sekunder mellan k√∂rningar (default: 300)")
_ = ap.add_argument("--skip-scripts", nargs="*", default=[],
                    help="Script att hoppa √∂ver")
_ = ap.add_argument("--no-scripts", action="store_true",
                    help="Hoppa √∂ver alla processing-scripts")
_ = ap.add_argument("--no-clean", action="store_true",
                    help="Hoppa √∂ver rensning (rekommenderat p√• Render)")

args = ap.parse_args()

# ‚îÄ‚îÄ 4. K√∂r vald handling ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
if __name__ == "__main__":
    print("üöÄ Startar automatisk scraping...")
    
    # TV√ÖNGSM√ÑSSIG CLEANUP F√ñRST (p√• Render)
    force_cleanup_on_start()
    
    # P√• Render, starta scraping i bakgrund och webserver som huvudprocess
    if IS_RENDER:
        # Scraping i bakgrundsprocess ist√§llet f√∂r webserver i thread
        def run_scraping_background():
            """K√∂r scraping i bakgrundsprocess"""
            print("üîÑ Startar scraping i bakgrundsprocess...")
            time.sleep(15)  # V√§nta s√• webservern hinner starta f√∂rst
            
            start = args.date or STARTING_DATE
            run_loop(
                start, 
                args.url, 
                args.db, 
                args.sleep, 
                args.workers,
                args.skip_scripts, 
                args.no_scripts, 
                args.no_clean,
                in_thread=True  # Viktigt: S√§g att detta k√∂rs i thread!
            )
        
        # Starta scraping i bakgrund
        import threading
        scraping_thread = threading.Thread(target=run_scraping_background, daemon=True)
        scraping_thread.start()
        print("‚úÖ Scraping startad i bakgrund")
        
        # Webserver som HUVUDPROCESS (det som Render √∂vvakar)
        print("üåê Startar webserver som huvudprocess...")
        import uvicorn
        port = int(os.environ.get("PORT", 8000))
        
        # Skapa databaser f√∂rst
        try:
            from utils.paths import POKER_DB, HEAVY_DB
            import sqlite3
            
            for db_path in [POKER_DB, HEAVY_DB]:
                if not db_path.exists():
                    print(f"üì¶ Skapar tom databas: {db_path}")
                    db_path.parent.mkdir(parents=True, exist_ok=True)
                    conn = sqlite3.connect(str(db_path))
                    conn.close()
        except Exception as e:
            print(f"‚ö†Ô∏è  Databas-skapande fel: {e}")
        
        print(f"üåê Webserver k√∂r som huvudprocess p√• port {port}")
        print(f"üîó URL: https://promethius.onrender.com")
        
        # K√ñR WEBSERVER SOM HUVUDPROCESS - inget threading!
        uvicorn.run(
            "app:app",
            host="0.0.0.0",
            port=port,
            reload=False,
            log_level="info",
            access_log=True
        )
    else:
        # Lokal utveckling - k√∂r scraping direkt
        start = args.date or STARTING_DATE
        run_loop(
            start, 
            args.url, 
            args.db, 
            args.sleep, 
            args.workers,
            args.skip_scripts, 
            args.no_scripts, 
            args.no_clean,
            in_thread=False
        )
