#!/usr/bin/env python3
# main.py â€“ central startpunkt som fÃ¶rst rensar och sedan startar scraping
# PÃ¥ Render startar ocksÃ¥ webservern fÃ¶r att ha allt i en robust process

from __future__ import annotations
import argparse
import sys
import os
import time
import datetime
import subprocess
import sqlite3
from pathlib import Path
import signal
import threading
from typing import Any
import psutil  # FÃ¶r process-hantering
import socket

# Import centraliserad path-hantering
sys.path.append(str(Path(__file__).resolve().parent))
from utils.paths import PROJECT_ROOT, POKER_DB, IS_RENDER

# â”€â”€ 1. Hitta projektroten och fÃ¶rbered import â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ROOT = PROJECT_ROOT
DB_PATH = POKER_DB.relative_to(ROOT) if not IS_RENDER else POKER_DB

print(f"ğŸ  Projektrot: {ROOT}")
print(f"ğŸ’¾ Database: {POKER_DB}")

# â”€â”€ CLEANUP-FUNKTIONER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def kill_old_processes():
    """DÃ¶dÃ¤r gamla Python-processer som kÃ¶r scraping/webserver"""
    if not IS_RENDER:
        return  # Bara pÃ¥ Render
        
    try:
        current_pid = os.getpid()
        killed_count = 0
        
        for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
            try:
                # Skippa vÃ¥r egen process
                if proc.info['pid'] == current_pid:
                    continue
                    
                # Leta efter andra Python-processer som kÃ¶r vÃ¥ra scripts
                if (proc.info['name'] in ['python', 'python3', 'python.exe'] and 
                    proc.info['cmdline'] and 
                    any('main.py' in str(cmd) or 'scrape.py' in str(cmd) or 'app.py' in str(cmd) 
                        for cmd in proc.info['cmdline'])):
                    
                    print(f"ğŸ”ª DÃ¶dÃ¤r gammal process: PID {proc.info['pid']} - {' '.join(proc.info['cmdline'][:3])}")
                    proc.terminate()
                    killed_count += 1
                    
            except (psutil.NoSuchProcess, psutil.AccessDenied):
                continue
                
        if killed_count > 0:
            print(f"âœ… DÃ¶dade {killed_count} gamla processer")
            time.sleep(2)  # VÃ¤nta sÃ¥ processer hinner dÃ¶
        else:
            print("âœ… Inga gamla processer att dÃ¶da")
            
    except Exception as e:
        print(f"âš ï¸  Kunde inte dÃ¶da gamla processer: {e}")

def cleanup_database_locks():
    """Rensar SQLite WAL/SHM-filer och stÃ¤nger lÃ¥sningar - HITTAR ALLA DATABASER"""
    try:
        from utils.paths import POKER_DB, HEAVY_DB, DB_DIR
        
        print("ğŸ” SÃ¶ker efter ALLA databas-filer rekursivt...")
        
        # Lista Ã¶ver alla databas-relaterade filer (kÃ¤nda paths)
        known_db_files = [
            POKER_DB,
            HEAVY_DB,
            POKER_DB.with_suffix('.db-wal'),
            POKER_DB.with_suffix('.db-shm'), 
            HEAVY_DB.with_suffix('.db-wal'),
            HEAVY_DB.with_suffix('.db-shm'),
        ]
        
        # SÃ¶k rekursivt efter ALLA databas-filer i hela data-omrÃ¥det
        search_dirs = [DB_DIR]
        if IS_RENDER:
            search_dirs.append(Path('/var/data'))
        
        all_db_files = []
        for search_dir in search_dirs:
            if search_dir.exists():
                # Hitta alla filer som matchar vÃ¥ra databas-namn
                patterns = ['poker.db*', 'heavy_analysis.db*']
                for pattern in patterns:
                    all_db_files.extend(search_dir.rglob(pattern))
        
        # Kombinera kÃ¤nda + hittade filer
        unique_db_files = list(set(known_db_files + all_db_files))
        
        print(f"   ğŸ“ Hittade {len(unique_db_files)} databas-relaterade filer")
        for db_file in unique_db_files:
            if db_file.exists():
                print(f"      â€¢ {db_file}")
        
        # FÃ¶rsÃ¶k stÃ¤nga alla SQLite-connections fÃ¶rst (bara .db-filer)
        for db_file in unique_db_files:
            if db_file.suffix == '.db' and db_file.exists():
                try:
                    # Ã–ppna kort connection fÃ¶r att trigga WAL checkpoint
                    conn = sqlite3.connect(str(db_file))
                    conn.execute("PRAGMA wal_checkpoint(TRUNCATE)")
                    conn.close()
                    print(f"   âœ“ Checkpoint: {db_file.name}")
                except Exception as e:
                    print(f"   âš ï¸  Checkpoint misslyckades fÃ¶r {db_file.name}: {e}")
        
        # Radera WAL/SHM-filer som kan vara lÃ¥sta
        removed_count = 0
        for db_file in unique_db_files:
            if db_file.name.endswith(('.db-wal', '.db-shm')) and db_file.exists():
                try:
                    db_file.unlink()
                    print(f"   âœ“ Raderade lÃ¥st fil: {db_file}")
                    removed_count += 1
                except Exception as e:
                    print(f"   âš ï¸  Kunde inte radera {db_file}: {e}")
        
        if removed_count > 0:
            print(f"âœ… Rensade {removed_count} lÃ¥sta databas-filer")
        else:
            print("âœ… Inga lÃ¥sta databas-filer att rensa")
            
    except Exception as e:
        print(f"âš ï¸  Fel vid databas-rensning: {e}")

def force_cleanup_on_start():
    """TvÃ¥ngsmÃ¤ssig cleanup vid start - dÃ¶dÃ¤r allt som kan stÃ¶ra"""
    if not IS_RENDER:
        return
        
    print("ğŸ§¹ TVÃ…NGSRENSNING VID START (Render)")
    
    # 1. DÃ¶da gamla processer fÃ¶rst
    kill_old_processes()
    
    # 2. Rensa databas-lÃ¥sningar
    cleanup_database_locks()
    
    # 3. Extra vÃ¤ntetid fÃ¶r att allt ska hinna "sÃ¤tta sig"
    print("â±ï¸  VÃ¤ntar 5 sekunder sÃ¥ allt hinner rensas...")
    time.sleep(5)
    
    print("âœ… TvÃ¥ngsrensning klar - fortsÃ¤tter med normal start")

os.chdir(ROOT)
from scrape_hh import scrape  # type: ignore[reportMissingImports]  # noqa: E402

# â”€â”€ 2. HjÃ¤lpfunktioner â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_config() -> dict[str, str]:
    kv: dict[str, str] = {}
    with open(ROOT / "config.txt", encoding="utf-8") as fh:
        for line in fh:
            if "=" in line:
                k, v = line.strip().split("=", 1)
                kv[k.strip().upper()] = v.strip()
    return kv

CFG = load_config()
STARTING_DATE = CFG["STARTING_DATE"]

print(f"ğŸŒ API: {CFG['BASE_URL']}")
print(f"   Organizer: {CFG['ORGANIZER']}")
print(f"   Event: {CFG['EVENT']}")
print()

def run_clean_start(skip_on_render: bool = True) -> bool:
    # PÃ¥ Render, skippa rensning om inte explicit begÃ¤rt
    if IS_RENDER and skip_on_render:
        print("ğŸš€ PÃ¥ Render - hoppar Ã¶ver rensning (anvÃ¤nd skip_on_render=False fÃ¶r att tvinga)")
        return True
        
    try:
        print("ğŸ§¹ KÃ¶r rensning...")
        result = subprocess.run([sys.executable, "clean_start.py"],
                                cwd=ROOT, capture_output=True, text=True, timeout=60)

        if result.returncode == 0:
            print("âœ… Rensning klar")
            if result.stdout.strip():
                print(f"   {result.stdout.strip()}")
            return True
        else:
            print(f"âŒ Rensning misslyckades: {result.stderr}")
            return False
    except subprocess.TimeoutExpired:
        print("â° Rensning tog fÃ¶r lÃ¥ng tid - avbryter")
        return False
    except Exception as e:
        print(f"âŒ Fel vid rensning: {e}")
        return False

def run_fetch_process(date_str: str, url: str | None, db: str | None,
                      skip_scripts: list[str] | None = None, no_scripts: bool = False) -> None:
    try:
        argv = ["scrape.py", date_str]
        if url:
            argv += ["--url", url]
        if db:
            argv += ["--db", db]
        if skip_scripts:
            argv += ["--skip-scripts"] + skip_scripts
        if no_scripts:
            argv += ["--no-scripts"]

        original_argv = sys.argv[:]
        sys.argv = argv

        scrape.main()

        sys.argv = original_argv

    except KeyboardInterrupt:
        print(f"\nâ¹ï¸  Scraping avbrutet fÃ¶r {date_str}")
        raise
    except Exception as e:
        print(f"âŒ Fel vid scraping fÃ¶r {date_str}: {e}")
        raise

def run_single_fetch(date_str: str, url: str | None, db: str | None,
                     skip_scripts: list[str] | None = None, no_scripts: bool = False) -> None:
    if not run_clean_start():
        print("âŒ Kan inte fortsÃ¤tta utan lyckad rensning")
        return

    run_fetch_process(date_str, url, db, skip_scripts, no_scripts)

def run_loop(start_date: str, url: str | None, db: str | None,
             sleep_s: int = 300, max_workers: int = 1,
             skip_scripts: list[str] | None = None, no_scripts: bool = False,
             no_clean: bool = False, in_thread: bool = False) -> None:
    # Smart fÃ¶rsta-deploy-detektion pÃ¥ Render
    if IS_RENDER and not no_clean:
        from utils.paths import POKER_DB, HEAVY_DB, DB_DIR
        
        # Marker-fil fÃ¶r att veta om fÃ¶rsta deployen Ã¤r gjord
        marker_file = DB_DIR / ".first_deploy_done"
        
        if not marker_file.exists():
            print("ğŸ‰ FÃ–RSTA DEPLOYEN - rensar alla databaser fÃ¶r fresh start...")
            
            # Extra sÃ¤kerhet: DÃ¶da alla processer och rensa lÃ¥sningar
            kill_old_processes()
            cleanup_database_locks()
            
            # Hitta ALLA databas-filer rekursivt fÃ¶r total rensning
            print("ğŸ” SÃ¶ker efter ALLA databas-filer fÃ¶r total rensning...")
            search_dirs = [DB_DIR]
            if IS_RENDER:
                search_dirs.append(Path('/var/data'))
            
            all_db_files = []
            for search_dir in search_dirs:
                if search_dir.exists():
                    # Hitta alla filer som matchar vÃ¥ra databas-namn
                    patterns = ['poker.db*', 'heavy_analysis.db*']
                    for pattern in patterns:
                        all_db_files.extend(search_dir.rglob(pattern))
            
            # LÃ¤gg till kÃ¤nda filer ocksÃ¥
            known_db_files = [
                POKER_DB,
                HEAVY_DB,
                POKER_DB.with_suffix('.db-wal'),
                POKER_DB.with_suffix('.db-shm'),
                HEAVY_DB.with_suffix('.db-wal'),
                HEAVY_DB.with_suffix('.db-shm'),
            ]
            
            unique_db_files = list(set(known_db_files + all_db_files))
            
            print(f"   ğŸ“ Hittade {len(unique_db_files)} databas-filer att radera")
            
            deleted_count = 0
            for db_file in unique_db_files:
                if db_file.exists():
                    try:
                        db_file.unlink()
                        print(f"   âœ“ Raderade {db_file}")
                        deleted_count += 1
                    except Exception as e:
                        print(f"   âš ï¸  Kunde inte radera {db_file}: {e}")
            
            print(f"âœ… Raderade {deleted_count} databas-filer totalt")
            
            # KÃ¶r full rensning
            if not run_clean_start(skip_on_render=False):
                print("âŒ Kritisk: Kan inte starta utan lyckad fÃ¶rsta rensning")
                sys.exit(1)
            
            # Skapa marker-fil sÃ¥ vi vet att fÃ¶rsta deployen Ã¤r gjord
            marker_file.write_text(f"First deploy completed: {datetime.datetime.now().isoformat()}")
            print("âœ… FÃ¶rsta deployen klar - framtida restarts behÃ¥ller data")
            
        else:
            print("â™»ï¸  Inte fÃ¶rsta deployen - behÃ¥ller befintlig data (kontinuerlig drift)")
            # LÃ¤s nÃ¤r fÃ¶rsta deployen gjordes
            try:
                deploy_time = marker_file.read_text().strip()
                print(f"   {deploy_time}")
            except:
                pass
    elif not no_clean and not run_clean_start():
        # Lokal miljÃ¶ - respektera --no-clean flaggan
        print("âŒ Kan inte fortsÃ¤tta utan lyckad rensning")
        return

    day = datetime.date.fromisoformat(start_date)

    # Signal handling bara i main thread, inte i scraping thread
    if not in_thread:
        def signal_handler(signum: int, frame: Any) -> None:
            if IS_RENDER and signum == signal.SIGTERM:
                # PÃ¥ Render: GÃ¶r ordentlig cleanup innan vi avslutar
                print(f"\nğŸ›‘ Fick SIGTERM pÃ¥ Render - gÃ¶r ordentlig cleanup...")
                
                try:
                    # StÃ¤ng databas-connections
                    cleanup_database_locks()
                    print("âœ… Databas-cleanup klar")
                except:
                    pass
                
                print("âœ… Graceful shutdown klar - avslutar")
                sys.exit(0)
            elif signum == signal.SIGTERM:
                print(f"\nğŸ›‘ Fick SIGTERM - avslutar...")
                sys.exit(0)
            else:
                print(f"\nğŸ›‘ Fick signal {signum} - stÃ¤nger av gracefully...")
                sys.exit(0)

        signal.signal(signal.SIGINT, signal_handler)
        signal.signal(signal.SIGTERM, signal_handler)
    else:
        print("ğŸ”„ Scraping-thread: Hoppar Ã¶ver signal handling (bara main thread)")

    batch_size = CFG.get("BATCH_SIZE", "500")
    print(f"ğŸ”„ Startar loop frÃ¥n {start_date}")
    print(f"   Batch-storlek: {batch_size} hÃ¤nder per dag")

    while True:
        try:
            current_date = day.isoformat()

            # KÃ¶r scraping synkront fÃ¶r att undvika extra processfork och sÃ¤nka CPU-toppar
            print(f"ğŸ”„ Startar scraping fÃ¶r {current_date}...")
            run_fetch_process(current_date, url, db, skip_scripts, no_scripts)
            print(f"âœ… Scraping klar fÃ¶r {current_date}")

            day += datetime.timedelta(days=1)

            if day == datetime.date.today():
                print("ğŸ•‘ VÃ¤ntar 10 minuter innan nÃ¤sta kÃ¶rning...")
                time.sleep(600)
            else:
                print(f"ğŸ•‘ VÃ¤ntar {sleep_s//60} min innan nÃ¤sta kÃ¶rning...")
                time.sleep(sleep_s)

        except KeyboardInterrupt:
            print("\nâ¹ï¸  Loop avbruten av anvÃ¤ndare")
            break
        except Exception as e:
            print(f"âŒ Fel i loop: {e}")
            print(f"â­ï¸  Hoppar Ã¶ver {day.isoformat()} och fortsÃ¤tter...")
            day += datetime.timedelta(days=1)
            time.sleep(5)

# â”€â”€ 3. CLI-parse â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ap = argparse.ArgumentParser(
    description="Automatisk poker scraping system"
)
_ = ap.add_argument("--date", help="Startdatum (default = STARTING_DATE frÃ¥n config.txt)")
_ = ap.add_argument("--url", help="Ã–verskriv BASE_URL")
_ = ap.add_argument("--db", help="Ã–verskriv DB-sÃ¶kvÃ¤g")
_ = ap.add_argument("--workers", type=int, default=1,
                    help="Antal worker-processer (default: 1)")
_ = ap.add_argument("--sleep", type=int, default=300,
                    help="Sovtid i sekunder mellan kÃ¶rningar (default: 300)")
_ = ap.add_argument("--skip-scripts", nargs="*", default=[],
                    help="Script att hoppa Ã¶ver")
_ = ap.add_argument("--no-scripts", action="store_true",
                    help="Hoppa Ã¶ver alla processing-scripts")
_ = ap.add_argument("--no-clean", action="store_true",
                    help="Hoppa Ã¶ver rensning (rekommenderat pÃ¥ Render)")

args = ap.parse_args()

# â”€â”€ 4. KÃ¶r vald handling â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    print("ğŸš€ Startar automatisk scraping...")
    
    # TVÃ…NGSMÃ„SSIG CLEANUP FÃ–RST (pÃ¥ Render)
    force_cleanup_on_start()
    
    # PÃ¥ Render, starta scraping i bakgrund och webserver som huvudprocess
    if IS_RENDER:
        # Scraping i bakgrundsprocess istÃ¤llet fÃ¶r webserver i thread
        def run_scraping_background():
            """KÃ¶r scraping i bakgrundsprocess"""
            print("ğŸ”„ Startar scraping i bakgrundsprocess...")
            time.sleep(15)  # VÃ¤nta sÃ¥ webservern hinner starta fÃ¶rst
            
            start = args.date or STARTING_DATE
            run_loop(
                start, 
                args.url, 
                args.db, 
                args.sleep, 
                args.workers,
                args.skip_scripts, 
                args.no_scripts, 
                args.no_clean,
                in_thread=True  # Viktigt: SÃ¤g att detta kÃ¶rs i thread!
            )
        
        # Starta scraping i bakgrund
        import threading
        scraping_thread = threading.Thread(target=run_scraping_background, daemon=True)
        scraping_thread.start()
        print("âœ… Scraping startad i bakgrund")
        
        # Webserver som HUVUDPROCESS (det som Render Ã¶vvakar)
        print("ğŸŒ Startar webserver som huvudprocess...")
        import uvicorn
        port = int(os.environ.get("PORT", 8000))
        
        # Skapa databaser fÃ¶rst
        try:
            from utils.paths import POKER_DB, HEAVY_DB
            import sqlite3
            
            for db_path in [POKER_DB, HEAVY_DB]:
                if not db_path.exists():
                    print(f"ğŸ“¦ Skapar tom databas: {db_path}")
                    db_path.parent.mkdir(parents=True, exist_ok=True)
                    conn = sqlite3.connect(str(db_path))
                    conn.close()
        except Exception as e:
            print(f"âš ï¸  Databas-skapande fel: {e}")
        
        print(f"ğŸŒ Webserver kÃ¶r som huvudprocess pÃ¥ port {port}")
        print(f"ğŸ”— URL: https://promethius.onrender.com")
        
        # KÃ–R WEBSERVER SOM HUVUDPROCESS - inget threading!
        uvicorn.run(
            "app:app",
            host="0.0.0.0",
            port=port,
            reload=False,
            log_level="info",
            access_log=True
        )
    else:
        # Lokal utveckling - kÃ¶r scraping direkt
        start = args.date or STARTING_DATE
        run_loop(
            start, 
            args.url, 
            args.db, 
            args.sleep, 
            args.workers,
            args.skip_scripts, 
            args.no_scripts, 
            args.no_clean,
            in_thread=False
        )
