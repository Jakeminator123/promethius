#!/usr/bin/env python3
# main.py â€“ central startpunkt som fÃ¶rst rensar och sedan startar scraping
# PÃ¥ Render startar ocksÃ¥ webservern fÃ¶r att ha allt i en robust process

from __future__ import annotations
import argparse
import sys
import os
import time
import datetime
import subprocess
from pathlib import Path
import signal
import threading
from typing import Any

# Import centraliserad path-hantering
sys.path.append(str(Path(__file__).resolve().parent))
from utils.paths import PROJECT_ROOT, POKER_DB, IS_RENDER

# â”€â”€ 1. Hitta projektroten och fÃ¶rbered import â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ROOT = PROJECT_ROOT
DB_PATH = POKER_DB.relative_to(ROOT) if not IS_RENDER else POKER_DB

print(f"ğŸ  Projektrot: {ROOT}")
print(f"ğŸ’¾ Database: {POKER_DB}")

os.chdir(ROOT)
from scrape_hh import scrape  # type: ignore[reportMissingImports]  # noqa: E402

def start_webserver_thread():
    """Startar webservern i en separat thread (bara pÃ¥ Render)"""
    if not IS_RENDER:
        return
        
    print("ğŸŒ Startar webserver-thread...")
    
    def run_webserver():
        try:
            # Skapa tomma databaser om de inte finns (pÃ¥ Render)
            if IS_RENDER:
                from utils.paths import POKER_DB, HEAVY_DB
                import sqlite3
                
                # Skapa minimala databaser om de inte finns
                for db_path in [POKER_DB, HEAVY_DB]:
                    if not db_path.exists():
                        print(f"ğŸ“¦ Skapar tom databas: {db_path}")
                        # Se till att parent directory finns
                        db_path.parent.mkdir(parents=True, exist_ok=True)
                        conn = sqlite3.connect(str(db_path))
                        conn.close()
                
                # Kontrollera frontend pÃ¥ Render
                frontend_path = Path(__file__).resolve().parent / "frontend" / "dist"
                if frontend_path.exists():
                    index_file = frontend_path / "index.html"
                    if index_file.exists():
                        print(f"âœ… Frontend byggd: {frontend_path}")
                    else:
                        print(f"âŒ index.html saknas: {index_file}")
                else:
                    print(f"âŒ Frontend dist saknas: {frontend_path}")
                    print("âš ï¸  Frontend kanske inte byggdes korrekt i Build Command")
            
            # Importera direkt istÃ¤llet fÃ¶r subprocess
            import uvicorn
            
            # Get port from environment variable (Render sets this) or default to 8000
            port = int(os.environ.get("PORT", 8000))
            print(f"ğŸŒ Webserver startar pÃ¥ port {port}...")
            print(f"ğŸ”— URL: https://promethius.onrender.com")
            
            # VÃ¤nta lite sÃ¥ databaserna hinner skapas
            time.sleep(2)
            
            # KÃ¶r uvicorn direkt i threaden
            uvicorn.run(
                "app:app",
                host="0.0.0.0",
                port=port,
                reload=False,  # Aldrig reload pÃ¥ Render
                log_level="info",
                access_log=True
            )
        except Exception as e:
            import traceback
            print(f"âŒ KRITISKT FEL - Webserver-thread krashade: {e}")
            print(f"ğŸ“‹ Traceback: {traceback.format_exc()}")
            print("ğŸ”„ FÃ¶rsÃ¶ker starta om webserver om 30 sekunder...")
            time.sleep(30)
            # Rekursiv restart
            run_webserver()
    
    web_thread = threading.Thread(target=run_webserver, daemon=False)  # INTE daemon!
    web_thread.start()
    
    # VÃ¤nta lÃ¤ngre sÃ¥ webservern hinner starta ordentligt  
    print("â±ï¸  VÃ¤ntar pÃ¥ att webserver ska starta...")
    time.sleep(10)
    print("âœ… Webserver-thread startad")

# â”€â”€ 2. HjÃ¤lpfunktioner â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_config() -> dict[str, str]:
    kv: dict[str, str] = {}
    with open(ROOT / "config.txt", encoding="utf-8") as fh:
        for line in fh:
            if "=" in line:
                k, v = line.strip().split("=", 1)
                kv[k.strip().upper()] = v.strip()
    return kv

CFG = load_config()
STARTING_DATE = CFG["STARTING_DATE"]

print(f"ğŸŒ API: {CFG['BASE_URL']}")
print(f"   Organizer: {CFG['ORGANIZER']}")
print(f"   Event: {CFG['EVENT']}")
print()

def run_clean_start(skip_on_render: bool = True) -> bool:
    # PÃ¥ Render, skippa rensning om inte explicit begÃ¤rt
    if IS_RENDER and skip_on_render:
        print("ğŸš€ PÃ¥ Render - hoppar Ã¶ver rensning (anvÃ¤nd skip_on_render=False fÃ¶r att tvinga)")
        return True
        
    try:
        print("ğŸ§¹ KÃ¶r rensning...")
        result = subprocess.run([sys.executable, "clean_start.py"],
                                cwd=ROOT, capture_output=True, text=True, timeout=60)

        if result.returncode == 0:
            print("âœ… Rensning klar")
            if result.stdout.strip():
                print(f"   {result.stdout.strip()}")
            return True
        else:
            print(f"âŒ Rensning misslyckades: {result.stderr}")
            return False
    except subprocess.TimeoutExpired:
        print("â° Rensning tog fÃ¶r lÃ¥ng tid - avbryter")
        return False
    except Exception as e:
        print(f"âŒ Fel vid rensning: {e}")
        return False

def run_fetch_process(date_str: str, url: str | None, db: str | None,
                      skip_scripts: list[str] | None = None, no_scripts: bool = False) -> None:
    try:
        argv = ["scrape.py", date_str]
        if url:
            argv += ["--url", url]
        if db:
            argv += ["--db", db]
        if skip_scripts:
            argv += ["--skip-scripts"] + skip_scripts
        if no_scripts:
            argv += ["--no-scripts"]

        original_argv = sys.argv[:]
        sys.argv = argv

        scrape.main()

        sys.argv = original_argv

    except KeyboardInterrupt:
        print(f"\nâ¹ï¸  Scraping avbrutet fÃ¶r {date_str}")
        raise
    except Exception as e:
        print(f"âŒ Fel vid scraping fÃ¶r {date_str}: {e}")
        raise

def run_single_fetch(date_str: str, url: str | None, db: str | None,
                     skip_scripts: list[str] | None = None, no_scripts: bool = False) -> None:
    if not run_clean_start():
        print("âŒ Kan inte fortsÃ¤tta utan lyckad rensning")
        return

    run_fetch_process(date_str, url, db, skip_scripts, no_scripts)

def run_loop(start_date: str, url: str | None, db: str | None,
             sleep_s: int = 300, max_workers: int = 1,
             skip_scripts: list[str] | None = None, no_scripts: bool = False,
             no_clean: bool = False) -> None:
    # Smart fÃ¶rsta-deploy-detektion pÃ¥ Render
    if IS_RENDER and not no_clean:
        from utils.paths import POKER_DB, HEAVY_DB, DB_DIR
        
        # Marker-fil fÃ¶r att veta om fÃ¶rsta deployen Ã¤r gjord
        marker_file = DB_DIR / ".first_deploy_done"
        
        if not marker_file.exists():
            print("ğŸ‰ FÃ–RSTA DEPLOYEN - rensar alla databaser fÃ¶r fresh start...")
            
            # Lista Ã¶ver databasfiler att radera
            db_files = [
                POKER_DB,
                HEAVY_DB,
                # WAL och SHM filer
                POKER_DB.with_suffix('.db-wal'),
                POKER_DB.with_suffix('.db-shm'),
                HEAVY_DB.with_suffix('.db-wal'),
                HEAVY_DB.with_suffix('.db-shm'),
            ]
            
            for db_file in db_files:
                if db_file.exists():
                    try:
                        db_file.unlink()
                        print(f"   âœ“ Raderade {db_file.name}")
                    except Exception as e:
                        print(f"   âš ï¸  Kunde inte radera {db_file.name}: {e}")
            
            # KÃ¶r full rensning
            if not run_clean_start(skip_on_render=False):
                print("âŒ Kritisk: Kan inte starta utan lyckad fÃ¶rsta rensning")
                sys.exit(1)
            
            # Skapa marker-fil sÃ¥ vi vet att fÃ¶rsta deployen Ã¤r gjord
            marker_file.write_text(f"First deploy completed: {datetime.datetime.now().isoformat()}")
            print("âœ… FÃ¶rsta deployen klar - framtida restarts behÃ¥ller data")
            
        else:
            print("â™»ï¸  Inte fÃ¶rsta deployen - behÃ¥ller befintlig data (kontinuerlig drift)")
            # LÃ¤s nÃ¤r fÃ¶rsta deployen gjordes
            try:
                deploy_time = marker_file.read_text().strip()
                print(f"   {deploy_time}")
            except:
                pass
    elif not no_clean and not run_clean_start():
        # Lokal miljÃ¶ - respektera --no-clean flaggan
        print("âŒ Kan inte fortsÃ¤tta utan lyckad rensning")
        return

    day = datetime.date.fromisoformat(start_date)

    def signal_handler(signum: int, frame: Any) -> None:
        if IS_RENDER and signum == signal.SIGTERM:
            # PÃ¥ Render ignorerar vi SIGTERM fÃ¶r kontinuerlig drift
            print(f"\nğŸ›¡ï¸  Fick SIGTERM pÃ¥ Render - fortsÃ¤tter kÃ¶ra (kontinuerlig drift)")
            return
        print(f"\nğŸ›‘ Fick signal {signum} - stÃ¤nger av gracefully...")
        sys.exit(0)

    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)

    batch_size = CFG.get("BATCH_SIZE", "500")
    print(f"ğŸ”„ Startar loop frÃ¥n {start_date}")
    print(f"   Batch-storlek: {batch_size} hÃ¤nder per dag")

    while True:
        try:
            current_date = day.isoformat()

            # KÃ¶r scraping synkront fÃ¶r att undvika extra processfork och sÃ¤nka CPU-toppar
            print(f"ğŸ”„ Startar scraping fÃ¶r {current_date}...")
            run_fetch_process(current_date, url, db, skip_scripts, no_scripts)
            print(f"âœ… Scraping klar fÃ¶r {current_date}")

            day += datetime.timedelta(days=1)

            if day == datetime.date.today():
                print("ğŸ•‘ VÃ¤ntar 10 minuter innan nÃ¤sta kÃ¶rning...")
                time.sleep(600)
            else:
                print(f"ğŸ•‘ VÃ¤ntar {sleep_s//60} min innan nÃ¤sta kÃ¶rning...")
                time.sleep(sleep_s)

        except KeyboardInterrupt:
            print("\nâ¹ï¸  Loop avbruten av anvÃ¤ndare")
            break
        except Exception as e:
            print(f"âŒ Fel i loop: {e}")
            print(f"â­ï¸  Hoppar Ã¶ver {day.isoformat()} och fortsÃ¤tter...")
            day += datetime.timedelta(days=1)
            time.sleep(5)

# â”€â”€ 3. CLI-parse â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
ap = argparse.ArgumentParser(
    description="Automatisk poker scraping system"
)
_ = ap.add_argument("--date", help="Startdatum (default = STARTING_DATE frÃ¥n config.txt)")
_ = ap.add_argument("--url", help="Ã–verskriv BASE_URL")
_ = ap.add_argument("--db", help="Ã–verskriv DB-sÃ¶kvÃ¤g")
_ = ap.add_argument("--workers", type=int, default=1,
                    help="Antal worker-processer (default: 1)")
_ = ap.add_argument("--sleep", type=int, default=300,
                    help="Sovtid i sekunder mellan kÃ¶rningar (default: 300)")
_ = ap.add_argument("--skip-scripts", nargs="*", default=[],
                    help="Script att hoppa Ã¶ver")
_ = ap.add_argument("--no-scripts", action="store_true",
                    help="Hoppa Ã¶ver alla processing-scripts")
_ = ap.add_argument("--no-clean", action="store_true",
                    help="Hoppa Ã¶ver rensning (rekommenderat pÃ¥ Render)")

args = ap.parse_args()

# â”€â”€ 4. KÃ¶r vald handling â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    print("ğŸš€ Startar automatisk scraping...")
    
    # PÃ¥ Render, starta webservern fÃ¶rst
    if IS_RENDER:
        # FÃ¶rsÃ¶k med threading fÃ¶rst
        try:
            start_webserver_thread()
            print("ğŸŒ Render: Webserver + Scraping i samma process fÃ¶r maximal stabilitet")
        except Exception as e:
            print(f"âŒ Threading misslyckades: {e}")
            print("ğŸ”„ Startar webserver direkt istÃ¤llet...")
            
            # Backup: Starta webservern direkt utan scraping
            import uvicorn
            port = int(os.environ.get("PORT", 8000))
            print(f"ğŸŒ Backup: Startar webserver direkt pÃ¥ port {port}")
            uvicorn.run("app:app", host="0.0.0.0", port=port, reload=False)
            exit()  # Om vi nÃ¥r hit kÃ¶rdes aldrig scraping
    
    # Scraping-loop (kÃ¶rs bara om webserver startade i thread)
    start = args.date or STARTING_DATE
    run_loop(
        start, 
        args.url, 
        args.db, 
        args.sleep, 
        args.workers,
        args.skip_scripts, 
        args.no_scripts, 
        args.no_clean
    )
